<!DOCTYPE html>
<!-- saved from url=(0042)https://gangw.cs.illinois.edu/class/cs463/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>cs589</title>
    <!--<meta name="viewport" content="width=device-width, initial-scale=1.0">-->
    <meta name="description" content="">
    <meta name="author" content="">
    
    <link href="http://liususan091219.github.io/teaching/cs589_23f/cs589_files/bootstrap.css" type="text/css" rel="stylesheet" media="screen">
   <link href="http://liususan091219.github.io/teaching/cs589_23f/cs589_files/non-responsive.css" rel="stylesheet">
   
    
<style type="text/css">
  body {
    padding-top: 0px;
    padding-bottom: 0px;
  }
  .sidebar-nav {
    padding: 0px 0;
  }
  .page-header {
    border-bottom: 1px solid #ADA9A9;
    margin-bottom: 10px;
  }
  
</style>

<style type="text/css">
</style>

</head>
 

<body style="">
<div class="container">  
    <div class="row">
    <div class="col-lg-12">

      <div class="page-header">
      <h2><b>CS 589: Text Mining and Information Retrieval </b></h2>
      <p> 
      <!--
      <a href="index.html">Home</a> |       
      <a href="project.html">Project</a>|
      -->
      <a href="http://liususan091219.github.io/teaching//cs589.html">Home</a> |
      <a href="https://sit.instructure.com/courses/35995/pages/homepage">Canvas</a> |
      <a href="http://liususan091219.github.io/teaching/cs589_23f/resources.html">Resources</a> |
      <a href="http://liususan091219.github.io/teaching/cs589_23f/faq.html">FAQ</a> 
      
      </p>

      </div>

      <div class="bs-component">

      <table class="table table-bordered table-striped" style="width: 60%">
        <colgroup>
       <col span="1" style="width: 30%;">
       <col span="1" style="width: 70%;">
    </colgroup>
        <tbody>
        <tr>
        <td>Instructor</td> <td><a href="http://liususan091219.github.io/index.html">Susan (Xueqing) Liu</a> (xueqing.liu AT stevens DOT edu)</td>
        </tr>
        <tr>
        <td>Meeting</td> 
        <td>
          <table>
            <tr>

      <ul style="list-style-type:none;padding-left: 0;">
        <li> Monday 6:30-9:00PM, Kidde 228 </li>
      </ul>
      </tr>
    </table>
        </td>
        </tr>
                <tr>
        <td>Susan's Office Hour</td> 
        <td>
          <table>
            <tr>

      <ul style="list-style-type:none;padding-left: 0;">
        <li> Mon: 2:00-4:00, GS 250 and Discord</li>
      </ul>
      </tr>
    </table>
        </td>
        </tr>
        <tr>
        <td>TA</td> 
        <td>
          <table>
            <tr>

      <ul style="list-style-type:none;padding-left: 0;">
        <li>Vrund Patel, OH: Wed 2-3 (Hybrid)</li>
        <li>Akshay Atam, OH: Fri 2-3 (Discord)</li>
        <li><a href="https://stevens.zoom.us/j/676333523"></a> </li>
        <li></li>
      </ul>
      </tr>
    </table>
        </td>
        
        </tr>
        </tbody>
        </table> 

<!--
      <p style="color:red;"><b>The information below is based on the previous offering of this class (for reference only). </br>
      The information is subject to potential changes before the start of the Spring 2020 semester.</b></p>
-->

  <h3><b>Class Description</b> </h3>

      <p>This course is a graduate-level course on fundamental techniques in information retrieval and text mining. By taking this course, students learn how to crawl, clean, process, mine, and infer knowledge from a massive amount of text data; how to build a search engine from scratch, including indexing, building retrieval models, and evaluating the performance of a search engine; they will also learn important machine learning and deep learning techniques for text data, including topic model, LSTM and BERT; finally, they will learn state-of-the-art research topics in text mining and information retrieval, and get research experience in these topics by working on the final project. 
      </p>

     <p><b>Learning Goals: </b>
      Upon successful completion of this course, students should be able to: 
      <ul>
      <li>Perform basic text processing tasks such as crawling data from the web, pre-processing and cleaning natural language sentences; </li>
      <li>Evaluate ranking algorithms by using information retrieval evaluation techniques, and implement text retrieval models such as TF-IDF and BM25;</li>
      <li>Use Elastic search to implement a prototypical search engine on Twitter data;
</li>
      <li>Derive inference algorithms for the maximum likelihood estimation (MLE), implement the expectation maximization (EM) algorithm;
</li>
      <li>Use state-of-the-art tools such as LSTM/Bert for text classification tasks;  </li>
    </ul>
      </p>

      <p><b>Recommended reading: </b>
      <ul>
        <li>
       <a href="https://www.amazon.com/Text-Data-Management-Analysis-Introduction/dp/197000116X"> Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining </a>. <i>ChengXiang Zhai and Sean Massung</i> </li>
        <li>
       <a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"> Introduction to information retrieval </a>. <i>Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch√ºtze</i> </li>
      </ul>
      </p>

<!--
<p><b>Important note: </b> It assumes a basic knowledge of the area such as the material covered by Computer Security I (CS 461). This semester it will expect ability to program in Java and C or C++.</p>


      <h3><b>Expected Work</b></h3>
      
      <p><b>Participation:</b> Students are required to attend all the lectures. Please inform the instructor and the TA via email if you cannot make it to the class due to travel or sickness. </p> 

      <p><b>Quizzes:</b> Students are expected to complete a short quiz within 24 hours after each lecture. The quiz contains 4-5 single-choice or multi-choice questions. The quiz is closely related to the respective lecture and is designed to be light-weighted (and hopefully fun) to improve student engagement during the lecture. Quiz is not graded --- students will receive points by simply trying them. </p> 

      <p><b>Machine Problem (MP):</b> Students are expected to work on 4-5 MPs throughout the semester. The projects will involve hand-on programming and data analysis, covering various topics that complement the lecture topics. Example topics include tracking user location based on social network data, interacting with Bitcoin APIs, multiple parties performing joint machine-learning without directly exchanging data. The list of project topics will be released later in Spring 2020. 
      </p><p><b>Survey paper:</b> This is for 4-credit student only. 
      </p>

      <h3><b>Get Ready for the Class</b></h3>
      <ul>
        <li>Make sure you have signed up to the <a href="https://piazza.com/class/k58g8mvy1vf28e">Piazza</a> group of this class </li>
        <li>Check if you have access to the <a href="https://learn.illinois.edu/course/view.php?id=43143">quiz release page</a> </li>
        
        <li>Create your git repo via this <a href="https://edu.cs.illinois.edu/create-ghe-repo/cs463-sp20/">one-time link</a></li>
        <li>Learn how to use git. Some helpful information <a href="https://piazza.com/class/k58g8mvy1vf28e?cid=7">here</a> </li>


        
      </ul> -->


       <h3><b>Schedule</b></h3>
      <a name="class" id="class"></a>
        Note: some slides require password, you can find the password from the home page in <a href="https://sit.instructure.com/courses/35995/pages/homepage"> Canvas</a><br><br>

        <table class="table table-bordered table-striped">
        <thead>
        <tr>
          <th>Date</th>
          <th>Slides/Readings</th>
          <th>Homework/Exams</th>
        </tr>
        </thead>
        <tbody>

        <tr><td>Week 1 of Sep 11</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
        History of IR, vector space model <a href="https://docs.google.com/presentation/d/1XdnsumeqBtDbzYwUXIv33t-RDsJ9znUT/edit?usp=sharing&ouid=111676164670170372109&rtpof=true&sd=true">slides</a></li></br>
        <li> Reading:  </li>
        <ul>
		<li>
        <a href="https://drive.google.com/file/d/1ZC4F9L7lukbw_tfnvb5oVevZTppHTYlP/view?usp=sharing">overview.ipynb</a></li>
        	<li> <a href = "https://nlp.stanford.edu/IR-book/pdf/06vect.pdf"> Scoring, term weighting and the
vector space model (lecture from Stanford IR course) </a> </li>
       <li> <a href = "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6182576"> The history of information retrieval research </a> </li>
     <li> <a href = "https://www.csee.umbc.edu/~nicholas/676/papers/p21-singhal.pdf">Pivoted length normalization </a> </li>
     <li> <a href = "https://dl.acm.org/doi/pdf/10.1145/1008992.1009004">A Formal Study of Information Retrieval Heuristics </a> </li>
   </ul>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
         <li> </li> 
       </ul>
       </td>

        <tr><td>Week 2 of Sep 18</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
       Probabilistic ranking principle, Probabilistic/LM-based retrieval <a href="https://docs.google.com/presentation/d/1WEgGYP9KufJBaLmHbKS4P4V8TQFC2kCA/edit?usp=sharing&ouid=111676164670170372109&rtpof=true&sd=true">slides</a>  </li></br>
        <li> Reading:  </li>

        <ul>
          <li>
        <a href="https://drive.google.com/file/d/1APqRRT41fLUcNtQv3IJ3WbEa7NXhAHK-/view?usp=sharing">lecture2.ipynb</a></li>
          <li> <a href = "https://www.emerald.com/insight/content/doi/10.1108/eb026683/full/html"> RSJ model without relevance judgment </a> </li>
          <li> <a href = "https://pdfs.semanticscholar.org/dfb7/b0fca3ecec86f0d57377d7dc3173835fdf67.pdf"> A Study of Retrieval Models for Long Documents and Queries in Information Retrieval (WWW 2016) </a> </li>
          <li> <a href = "http://ciir.cs.umass.edu/pubfiles/ir-120.pdf"> A language model approach to information retrieval </a> </li>
          <li> <a href = "http://sifaka.cs.uiuc.edu/course/498cxz06s/kldir.pdf"> Notes on the KL-divergence retrieval formula and Dirichlet prior smoothing </a> </li>
          <li> <a href = "http://maroo.cs.umass.edu/pub/web/getpdf.php?id=811"> Retrieval Models for Question and Answer Archives </a> </li>
          <li> <a href = "https://dl.acm.org/doi/10.1145/564376.564387"> Two-stage language models for information retrieval </a> </li>
          <li> <a href = "https://dl.acm.org/doi/10.1145/502585.502654"> Model-based feedback in the language modeling approach to information retrieval </a> </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
         <li> </li> 
       </ul>
       </td>
        </tr> 

        <tr><td>Week 3 of Sep 25</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
      <li>
        IR evaluation, relevance feedback <a href="https://docs.google.com/presentation/d/1jYL4QY8DUtOyvO7M8ZN2eikq3d151TLu/edit?usp=sharing&ouid=111676164670170372109&rtpof=true&sd=true">slides</a> </li><br>
         <li> Reading:  </li>
        <ul>
          <li>
        <a href="https://drive.google.com/file/d/1-yu6UljxfnSGSTsWnXNEITal-OSu-5Qd/view?usp=sharing">lecture3.ipynb</a></li>
        <li> <a href = "https://dl.acm.org/doi/pdf/10.1145/1645953.1646259"> A Comparative Study of Methods for Estimating Query
Language Models with Pseudo Feedback </a> </li>
<li> <a href = "https://dl.acm.org/doi/10.1145/1341531.1341545"> An experimental comparison of click position-bias models </a> </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
                  <li> <a style="color:blue;">HW1 due</a> Implementing IR models, evaluations to retrievel Stack Overflow posts</li> 
       </ul>
       </td>
        </tr> 

        <tr><td>Week 4 of Oct 2</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">

                <li>
         IR infrastructure: inverted index <a href="https://docs.google.com/presentation/d/1CArJWo8M8ZouzogawwRVSl_AY14mSAxW/edit?usp=sharing&ouid=111676164670170372109&rtpof=true&sd=true">slides</a>  </li> <li> ElasticSearch <a href="https://docs.google.com/presentation/d/1KKQ8iHH-KzG_d384TbRpa9Ar5WAu6X53/edit?usp=sharing&ouid=111676164670170372109&rtpof=true&sd=true">slides</a> </li> <br>
        <li>
        Reading </li>
        <ul>
          <a href="https://drive.google.com/file/d/1potRJrsHhXRSrWqo9XDwe01ec5OlEe9x/view?usp=sharing">lecture4.ipynb</a></li>
        <li> <a href = "https://nlp.stanford.edu/IR-book/"> Introduction to information retrieval. Chapter 2-5. </a> </li>
<li> <a href = "http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf"> The PageRank Citation Ranking: Bringing Order to the Web </a> </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
           
       </ul>
       </td>
        </tr> 

        <tr><td>Week 5 of Oct 9 (Oct 10 class)</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
        Web search; learning to rank <a href="https://docs.google.com/presentation/d/1C3esbCVqw9im3JvNHb-UZHpaM1tE3fcI/edit#slide=id.p1">slides</a> <br>
        <li>
        Reading </li>
        <ul>
        <li> <a href = "https://colab.research.google.com/drive/1iijpzmqRyvd32w2MmW0eLJ8fhtJ7R_qi?usp=sharing"> Colab notebook on gradient boosted regression tree </a> </li>
        <li> <a href = "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.1288&rep=rep1&type=pdf"> An Experimental Comparison of Click Position-Bias Models </a> </li>
<li> <a href = "http://www.cse.msu.edu/~cse847/readings/prank_largemargin.pdf"> Ranking with Large Margin Principle: Two
Approaches </a> </li>
<li> <a href = "http://www.cs.cmu.edu/~pbennett/papers/burgesLearningToRank-2011.pdf"> Learning to Rank Using an Ensemble of Lambda-Gradient Models </a> </li>
<li> <a href = "https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf"> From RankNet to LambdaRank to
LambdaMART: An Overview </a> </li>
<li> <a href = "https://arxiv.org/abs/1603.02754"> XGBoost: A Scalable Tree Boosting System </a> </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
         <li> <a style="color:blue;">HW2 due</a> Using ElasticSearch to retrieve StackOverflow posts</li>   
       </ul>
       </td>
        </tr> 

        <tr><td>Week 6 of Oct 16</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
         Topic model, representing semantics <a href="https://docs.google.com/presentation/d/1Y2xrCIyMGZA2jA1eTAiPhDX6i5HhJXIw/edit?usp=sharing&ouid=111676164670170372109&rtpof=true&sd=true">slides</a>  </li><br>
      <li>
        Reading </li>
        <ul>
<li> <a href = "http://www-personal.umich.edu/~qmei/pub/kdd07-label.pdf"> Automatic Labeling of Multinomial Topic Models </a> </li>
<li> <a href = "https://arxiv.org/pdf/1212.3900.pdf"> Notes on Probabilistic Latent Semantic Analysis </a> </li>
<li> <a href = "https://www.biz.uiowa.edu/faculty/trietz/papers/ITMTF.pdf"> Mining Causal Topics in Text Data:
Iterative Topic Modeling with Time Series Feedback </a> </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">      
       </ul>
       </td>
        </tr> 

        <tr><td>Week 7 of Oct 23</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
        Word2vec, Recurrent Networks <a href="https://docs.google.com/presentation/d/1EYunLhv4Iqe-g-OY6kpoka1D9jDDJnZ5/edit?usp=sharing&ouid=111676164670170372109&rtpof=true&sd=true">slides</a> <br>
         <li>
        Reading </li>
        <ul>
        <li>
        <a href="https://colab.research.google.com/drive/16-aEjN9ma1S76GzdKWvy9ajthfyepjNs#scrollTo=4abbd4ea">wordembedding.ipynb</a></li>
        <li> <a href = "https://jmlr.org/papers/volume3/tmp/bengio03a.pdf"> A Neural Probabilistic Language Model </a> </li>
<li> <a href = "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"> Distributed Representations of Words and Phrases
and their Compositionality </a> </li>
<li> <a href = "https://nlp.stanford.edu/pubs/glove.pdf"> GloVe: Global Vectors for Word Representation </a> </li>
<li> <a href = "http://proceedings.mlr.press/v28/pascanu13.pdf"> On the difficulty of training recurrent neural networks </a> </li>
 </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
<li> Project paper review due </li> 
       </ul>
       </td>
        </tr> 

        <tr><td>Week 8 of Oct 30</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
        midterm  </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
         <li> <a style="color:red;">Midterm </a></li> 
         
       </ul>
       </td>
        </tr> 

        <tr><td>Week 9 of Nov 6</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li> Attention; Transformers <a href="https://drive.google.com/file/d/1VYyzsMGwklLHDWN_H2XKuEHhiosijsOR/view?usp=sharing">slides</a> </li><br>
         <li>
        Reading </li>
        <ul>
<li> <a href = "https://medium.com/lsc-psd/introduction-of-self-attention-layer-in-transformer-fc7bff63f3bc"> Introduction of Self-Attention Layer in Transformer </a> </li>
          <li> <a href = "https://arxiv.org/pdf/1508.04025.pdf"> Effective Approaches to Attention-based Neural Machine Translation </a> </li>

          <li> <a href = "https://arxiv.org/pdf/1409.0473.pdf"> NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE </a> </li>

          <li> <a href = "https://arxiv.org/pdf/1802.05365.pdf"> Deep contextualized word representations </a> </li>
          <li> <a href = "https://arxiv.org/abs/1706.03762"> Attention Is All You Need </a> </li>
          <li> <a href = "https://arxiv.org/abs/1810.04805"> BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding </a> </li>
          <li> <a href = "https://www.aclweb.org/anthology/N18-2074.pdf"> Self-Attention with Relative Position Representations </a> </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
         <li> <a style="color:blue;">HW3 due</a> deriving/implementing EM algorithm for StackOverflow word tagging </li> 
       </ul>
       </td>
        </tr> 

        <tr><td>Week 10 of Nov 13</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li> Frontier topic 1<a href="https://drive.google.com/file/d/1HcljUt70kQy6xAG9segSH63Bp6zuVnLz/view?usp=sharing">slides</a> </li><br>
         <li>
        Reading </li>
        <ul>
          <li> <a href = "https://arxiv.org/abs/1907.11692"> RoBERTa: A Robustly Optimized BERT Pretraining Approach </a> </li>
          <li> <a href = "https://arxiv.org/abs/1906.08237"> XLNet: Generalized Autoregressive Pretraining for Language Understanding </a> </li>
          <li> <a href = "https://arxiv.org/abs/1910.10683"> Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer </a> </li>
          <li> <a href = "https://arxiv.org/abs/2004.05150"> Longformer: The Long-Document Transformer </a> </li>
          <li> <a href = "https://arxiv.org/abs/2002.08155"> CodeBERT: A Pre-Trained Model for Programming and Natural Languages </a> </li>
          <li> <a href = "https://arxiv.org/abs/1903.10676"> SciBERT: A Pretrained Language Model for Scientific Text </a> </li>
          <li> <a href = "https://arxiv.org/abs/1901.08746"> BioBERT: a pre-trained biomedical language representation model for biomedical text mining </a> </li>
  
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
         <li> Project proposal due</li>
       </ul>
       </td>
        </tr> 

        <tr><td>Week 11 of Nov 20</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
         Frontier topic 2 <a href="https://drive.google.com/file/d/1HcljUt70kQy6xAG9segSH63Bp6zuVnLz/view?usp=sharing">slides</a> <br>
       </li><br>
         <li>
        Reading </li>
        <ul>
          <li> <a href = "https://rstudio-pubs-static.s3.amazonaws.com/337306_79a7966fad184532ab3ad66b323se96e.html"> Derivation for cross entropy gradient </a> </li>
          <li> <a href = "https://arxiv.org/abs/1503.02531"> Distilling the Knowledge in a Neural Network </a> </li>
          <li> <a href = "https://arxiv.org/abs/1909.11942"> ALBERT: A Lite BERT for Self-supervised Learning of Language Representations </a> </li>
          <li> <a href = "https://arxiv.org/abs/2001.04451"> Reformer: The Efficient Transformer </a> </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
          <li> <a style="color:blue;"> HW4 due</a> StackOverflow posts tag prediction </li> 
       </ul>
       </td>
        </tr> 

        <tr><td>Week 12 of Nov 27</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
       Frontier topic 3 </li><br>
         <li>
        Reading </li>
        <ul>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">

       </ul>
       </td>
        </tr> 

        <tr><td>Week 13 of Dec 4</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
       Project presentation </li><br>
         <li>
        Reading </li>
        <ul>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
       </ul>
       </td>
        </tr> 

        <tr><td>Week 14 of Dec 11</td> 
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
        <li>
       Project presentation </li>
      </ul>
        </td>
        <td>
        <ul style="list-style-type:none;padding-left: 0;">
       </ul>
       </td>
        </tr> 


        </tbody>
        </table> 



      
      <h3><b>Final Grade Calculator</b></h3>
      <p>
      <a name="grades" id="grades"></a>
      </p>


      <table class="table table-bordered table-striped" style="width: 40%">
        <col span="1" style="width: 50%;">
       <col span="1" style="width: 50%;">
        <tbody>
        <tr>
        <td>Homework</td><td>40%</td>
        </tr>
        <tr>
        <td>Midterm</td><td>25%</td>
        </tr>
        <tr>
        <td>Final</td> <td>30%</td>
        </tr>
        <tr>
        <td>Attendance</td> <td>5%</td>
        </tr>
        </tbody>
        </table> 

      <h3><b>Policies</b></h3>
      <p>
      <a name="pol" id="pol"></a>
      </p>

      <p><b>Late Policy: </b> submit within 24 hours of deadline - 90%, within 48 hours - 70%, over 48 hours - 0 point, 0 if code not compile.

      </p><p><b>Academic Integrity: </b>
      Students must follow the instructions from the (<a href="https://web.stevens.edu/honor/about.html">Stevens Honor system</a>). This course will have a zero-tolerance policy regarding plagiarism. You should complete all the assignments and quizzes on your own. 

    You can help your classmates with questions such as how to use the programming language, what the library classes or methods do, what the errors mean, and how to interpret the assignment instructions. 

  You are encouraged to come to both the instructor and the CAs' office hours regarding any questions you have, or email your questions to both the instructors or the CAs. However, you may not give or receive help from others (except the CA) with the actual implementation or answers for any of the assignments or tests. Do not show or share your code with others, and do not view or copy source code from others. For the same reason, you are not allowed to copy and paste a code snippet you found online in the assignments. 

All electronic work submitted for this course will be archived and subjected to automatic plagiarism detection. Whenever in doubt, please seek clarifications from the instructor. Students who violate the academic intergrity principle of Stevens will be immediately reported to the department and the school (which could leave a permanent mark on the transcript).</p>

<p>If you need special accommodations because of a disability, please contact the instructor through emails.</p>

<div class="page-header"></div>

      



        
    </div>
</div>

<p><a href="https://gangw.cs.illinois.edu/class/cs463/" target="_blank" style="color: gray">Thanks to Prof. Gang Wang for the website template</a></p>

<script src="./cs589_files/jquery-1.10.2.min.js"></script>
<script src="./cs589_files/bootstrap.min.js"></script>

</div></body></html>
